{
	"name": "SparkSQL_Load_Data_From_Blob_To_Synapse_SQL_Pool",
	"properties": {
		"description": "SparkSQL_Load_Data_From_Blob_To_Synapse_SQL_Pool",
		"folder": {
			"name": "Master Pipeline Notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2daaba1a-8ae7-4b0a-afeb-16c0ef02f55e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/87083017-2c2b-47c8-8ea7-b4d36686fc97/resourceGroups/Dev_ResourceGroup/providers/Microsoft.Synapse/workspaces/cmdevsynapse/bigDataPools/devpool",
				"name": "devpool",
				"type": "Spark",
				"endpoint": "https://cmdevsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Getiing the parameter value from the pipeline \r\n",
					"FolderName = \"2022_08\""
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Using the parameter value creating a new variables to use while reading the file\r\n",
					"FolderName_Without_Separator = FolderName.replace(\"_\", \"\")\r\n",
					"year = FolderName.split('_')[0]\r\n",
					"month = FolderName.split('_')[1]"
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.functions import lit,col\r\n",
					"\r\n",
					"# Creating a spark session and connection to blob storage account\r\n",
					"blob_account_name = \"rawdevstorage\"\r\n",
					"blob_container_name = \"rawdata\"\r\n",
					"sc = SparkSession.builder.getOrCreate()\r\n",
					"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
					"blob_sas_token = token_library.getConnectionString(\"cmdevsynapse-WorkspaceDefaultStorage\")\r\n",
					"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),blob_sas_token)"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create a dynamic column delimiter using regex for AMR\r\n",
					"import re\r\n",
					"def f_get_delimiter (source_path):\r\n",
					"    try:\r\n",
					"        headerlist = spark.sparkContext.textFile(source_path).take(1)\r\n",
					"        header_str = ''.join(headerlist)\r\n",
					"\r\n",
					"        results= re.search(\"(,|;|\\\\|)\",header_str)\r\n",
					"        return results.group()\r\n",
					"    except Exception as err:\r\n",
					"        print(\"Error Occured \", str(err))\r\n",
					"amr_Filename = 'abfss://rawdata@rawdevstorage.dfs.core.windows.net/%s/P6523_Verbruiken_AMR_GEMEENTE_%s.csv' % (FolderName, FolderName_Without_Separator)\r\n",
					"amr_delimiter_type = (f_get_delimiter(amr_Filename))\r\n",
					"\r\n",
					"# Reading the first File (Analogue Reading File)\r\n",
					"FileName = 'abfss://rawdata@rawdevstorage.dfs.core.windows.net/%s/P6523_Verbruiken_AMR_GEMEENTE_%s.csv' % (FolderName,FolderName_Without_Separator)\r\n",
					"df1 = spark.read.load(FileName, format='csv', header=True,delimiter=amr_delimiter_type,inferSchema=True)\r\n",
					"\r\n",
					"# Create a temporary view: To run SQL queries on a PySpark DataFrame, you need to create a temporary view using the createOrReplaceTempView method.\r\n",
					"df1.createOrReplaceTempView(\"AMR_Measurement_Reading\")"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Run SQL queries: Once you have created a temporary view, you can run SQL queries on it using the spark.sql method.\r\n",
					"# Example 2: remove duplicate from the temporarry view\r\n",
					"selected_df1 = spark.sql(\"SELECT Distinct * FROM AMR_Measurement_Reading\")\r\n",
					"\r\n",
					"# Recreating the view after removing the duplicates\r\n",
					"selected_df1.createOrReplaceTempView(\"AMR_Measurement_Reading\")\r\n",
					"\r\n",
					"# Selecting the required columns\r\n",
					"selected_df1 = spark.sql(\"SELECT Trekkingsdatum,Peildatum\\\r\n",
					",CAST(Peildatum as DATE) as Peildatum1\\\r\n",
					", Markt, Leveringsadres_Postcode, Leveringsadres_Gemeente\\\r\n",
					", WerkelijkVerbruik_RM_2017, BenaderendVerbruik_VM_2017, AantalToegangspunten_2017\\\r\n",
					", WerkelijkVerbruik_RM_2018, BenaderendVerbruik_VM_2018, AantalToegangspunten_2018\\\r\n",
					", WerkelijkVerbruik_RM_2019, BenaderendVerbruik_VM_2019, AantalToegangspunten_2019\\\r\n",
					", WerkelijkVerbruik_RM_2020, BenaderendVerbruik_VM_2020, AantalToegangspunten_2020\\\r\n",
					", WerkelijkVerbruik_RM_2021, BenaderendVerbruik_VM_2021, AantalToegangspunten_2021\\\r\n",
					",'Analogue' as MeasurementType\\\r\n",
					" FROM AMR_Measurement_Reading\")\r\n",
					"\r\n",
					"# Recreating the view after removing the duplicates\r\n",
					"selected_df1.createOrReplaceTempView(\"AMR_Measurement_Reading\")\r\n",
					"\r\n",
					"# Display the result: To display the result of the SQL query, you can use the show method.\r\n",
					"display(selected_df1.limit(5))"
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"SELECT Distinct Trekkingsdatum, Peildatum\r\n",
					",TO_DATE(Peildatum, 'MM/dd/yyyy') Peildatum1\r\n",
					", Markt, Leveringsadres_Postcode, Leveringsadres_Gemeente\r\n",
					", WerkelijkVerbruik_RM_2017, BenaderendVerbruik_VM_2017, AantalToegangspunten_2017\r\n",
					", WerkelijkVerbruik_RM_2018, BenaderendVerbruik_VM_2018, AantalToegangspunten_2018\r\n",
					", WerkelijkVerbruik_RM_2019, BenaderendVerbruik_VM_2019, AantalToegangspunten_2019\r\n",
					", WerkelijkVerbruik_RM_2020, BenaderendVerbruik_VM_2020, AantalToegangspunten_2020\r\n",
					", WerkelijkVerbruik_RM_2021, BenaderendVerbruik_VM_2021, AantalToegangspunten_2021 \r\n",
					"FROM AMR_Measurement_Reading\r\n",
					"LIMIT 5"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"\r\n",
					"# convert the date column to datetime\r\n",
					"try:\r\n",
					"    selected_df1['Peildatum'] = pd.to_datetime(selected_df1['Peildatum'], format='%d/%b/%y')\r\n",
					"except:\r\n",
					"    try:\r\n",
					"        selected_df1['Peildatum'] = pd.to_datetime(selected_df1['Peildatum'], format='%m/%d/%y')\r\n",
					"    except:\r\n",
					"        try:\r\n",
					"            selected_df1['Peildatum'] = pd.to_datetime(selected_df1['Peildatum'], format='%d/%b/%Y')\r\n",
					"        except:\r\n",
					"            selected_df1['Peildatum'] = pd.to_datetime(selected_df1['Peildatum'], format='%d-%b-%y')\r\n",
					"\r\n",
					"# convert the datetime object to the desired format\r\n",
					"selected_df1['Peildatum'] = selected_df1['Peildatum'].dt.strftime('%d-%m-%Y')\r\n",
					"\r\n",
					"selected_df1['BenaderendVerbruik_VM_2017_Norm'] = selected_df1[\"BenaderendVerbruik_VM_2017\"]\r\n",
					"selected_df1['BenaderendVerbruik_VM_2018_Norm'] = selected_df1[\"BenaderendVerbruik_VM_2018\"]\r\n",
					"selected_df1['BenaderendVerbruik_VM_2019_Norm'] = selected_df1[\"BenaderendVerbruik_VM_2019\"]\r\n",
					"selected_df1['BenaderendVerbruik_VM_2020_Norm'] = selected_df1[\"BenaderendVerbruik_VM_2020\"]\r\n",
					"selected_df1['BenaderendVerbruik_VM_2021_Norm'] = selected_df1[\"BenaderendVerbruik_VM_2021\"]\r\n",
					"\r\n",
					"selected_df1['WerkelijkVerbruik_RM_2017'] = selected_df1['WerkelijkVerbruik_RM_2017'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['WerkelijkVerbruik_RM_2018'] = selected_df1['WerkelijkVerbruik_RM_2018'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['WerkelijkVerbruik_RM_2019'] = selected_df1['WerkelijkVerbruik_RM_2019'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['WerkelijkVerbruik_RM_2020'] = selected_df1['WerkelijkVerbruik_RM_2020'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['WerkelijkVerbruik_RM_2021'] = selected_df1['WerkelijkVerbruik_RM_2021'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['Gemiddeld_WerkelijkVerbruik'] = ((selected_df1['WerkelijkVerbruik_RM_2017'].astype(float)\\\r\n",
					"+ selected_df1['WerkelijkVerbruik_RM_2018'].astype(float) + selected_df1['WerkelijkVerbruik_RM_2019'].astype(float) + selected_df1['WerkelijkVerbruik_RM_2020'].astype(float)\\\r\n",
					"+ selected_df1['WerkelijkVerbruik_RM_2021'].astype(float))/5)\r\n",
					"\r\n",
					"selected_df1['BenaderendVerbruik_VM_2017'] = selected_df1['BenaderendVerbruik_VM_2017'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2018'] = selected_df1['BenaderendVerbruik_VM_2018'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2019'] = selected_df1['BenaderendVerbruik_VM_2019'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2020'] = selected_df1['BenaderendVerbruik_VM_2020'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2021'] = selected_df1['BenaderendVerbruik_VM_2021'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['Gemiddeld_BenaderendVerbruik'] = ((selected_df1['BenaderendVerbruik_VM_2017'].astype(float)\\\r\n",
					"+ selected_df1['BenaderendVerbruik_VM_2018'].astype(float) + selected_df1['BenaderendVerbruik_VM_2019'].astype(float) + selected_df1['BenaderendVerbruik_VM_2020'].astype(float)\\\r\n",
					"+ selected_df1['BenaderendVerbruik_VM_2021'].astype(float))/5)\r\n",
					"\r\n",
					"selected_df1['BenaderendVerbruik_VM_2017_Norm'] = selected_df1['BenaderendVerbruik_VM_2017_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2018_Norm'] = selected_df1['BenaderendVerbruik_VM_2018_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2019_Norm'] = selected_df1['BenaderendVerbruik_VM_2019_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2020_Norm'] = selected_df1['BenaderendVerbruik_VM_2020_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['BenaderendVerbruik_VM_2021_Norm'] = selected_df1['BenaderendVerbruik_VM_2021_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df1['Gemiddeld_BenaderendVerbruik_Norm'] = ((selected_df1['BenaderendVerbruik_VM_2017_Norm'].astype(float)\\\r\n",
					"+ selected_df1['BenaderendVerbruik_VM_2018_Norm'].astype(float) + selected_df1['BenaderendVerbruik_VM_2019_Norm'].astype(float) + selected_df1['BenaderendVerbruik_VM_2020_Norm'].astype(float)\\\r\n",
					"+ selected_df1['BenaderendVerbruik_VM_2021_Norm'].astype(float))/5)\r\n",
					"\r\n",
					"selected_df1['Total_AantalToegangspunten'] = selected_df1['AantalToegangspunten_2017'].astype(float)\\\r\n",
					"+ selected_df1['AantalToegangspunten_2018'].astype(float) + selected_df1['AantalToegangspunten_2019'].astype(float) + selected_df1['AantalToegangspunten_2020'].astype(float)\\\r\n",
					"+ selected_df1['AantalToegangspunten_2021'].astype(float)\r\n",
					"\r\n",
					"# Rearrange the columns\r\n",
					"selected_df1 = selected_df1[['Trekkingsdatum', 'Peildatum', 'Markt', 'Leveringsadres_Postcode', 'Leveringsadres_Gemeente', 'WerkelijkVerbruik_RM_2017'\\\r\n",
					", 'BenaderendVerbruik_VM_2017', 'BenaderendVerbruik_VM_2017_Norm', 'AantalToegangspunten_2017', 'WerkelijkVerbruik_RM_2018', 'BenaderendVerbruik_VM_2018'\\\r\n",
					", 'BenaderendVerbruik_VM_2018_Norm', 'AantalToegangspunten_2018', 'WerkelijkVerbruik_RM_2019', 'BenaderendVerbruik_VM_2019', 'BenaderendVerbruik_VM_2019_Norm'\\\r\n",
					", 'AantalToegangspunten_2019', 'WerkelijkVerbruik_RM_2020', 'BenaderendVerbruik_VM_2020', 'BenaderendVerbruik_VM_2020_Norm', 'AantalToegangspunten_2020'\\\r\n",
					", 'WerkelijkVerbruik_RM_2021', 'BenaderendVerbruik_VM_2021', 'BenaderendVerbruik_VM_2021_Norm', 'AantalToegangspunten_2021'\\\r\n",
					",'Gemiddeld_WerkelijkVerbruik','Gemiddeld_BenaderendVerbruik','Gemiddeld_BenaderendVerbruik_Norm','Total_AantalToegangspunten'\\\r\n",
					", 'MeasurementType']] #, 'MeasurementYear', 'MeasurementMonth'"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Checking the column names\r\n",
					"selected_df1.columns"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Selecting the top 5 rows\r\n",
					"selected_df1.head()"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create a dynamic column delimiter using regex for DM\r\n",
					"import re\r\n",
					"def f_get_delimiter (source_path):\r\n",
					"    try:\r\n",
					"        headerlist = spark.sparkContext.textFile(source_path).take(1)\r\n",
					"        header_str = ''.join(headerlist)\r\n",
					"\r\n",
					"        results= re.search(\"(,|;|\\\\|)\",header_str)\r\n",
					"        return results.group()\r\n",
					"    except Exception as err:\r\n",
					"        print(\"Error Occured \", str(err))\r\n",
					"amr_Filename = 'abfss://rawdata@rawdevstorage.dfs.core.windows.net/%s/P6523_Verbruiken_DM_GEMEENTE_%s.csv' % (FolderName, FolderName_Without_Separator)\r\n",
					"amr_delimiter_type = (f_get_delimiter(amr_Filename))"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"\r\n",
					"# Reading the second File (Digital Reading File)\r\n",
					"FileName = 'abfss://rawdata@rawdevstorage.dfs.core.windows.net/%s/P6523_Verbruiken_DM_GEMEENTE_%s.csv' % (FolderName,FolderName_Without_Separator)\r\n",
					"df2 = spark.read.load(FileName, format='csv', header=True,delimiter=amr_delimiter_type,inferSchema=True)\r\n",
					"\r\n",
					"# Convert to pandas data frame\r\n",
					"df2 = df2.select(\"*\").toPandas()\r\n",
					"\r\n",
					"# Dropping the duplicates\r\n",
					"df2 = df2.drop_duplicates()\r\n",
					"\r\n",
					"# Selecting the required columns\r\n",
					"selected_df2 = df2[['Trekkingsdatum', 'Peildatum', 'Markt', 'Leveringsadres_Postcode', 'Leveringsadres_Gemeente'\\\r\n",
					", 'WerkelijkVerbruik_RM_2017', 'BenaderendVerbruik_VM_2017', 'BenaderendVerbruik_VM_2017_Norm', 'AantalToegangspunten_2017'\\\r\n",
					", 'WerkelijkVerbruik_RM_2018', 'BenaderendVerbruik_VM_2018', 'BenaderendVerbruik_VM_2018_Norm', 'AantalToegangspunten_2018'\\\r\n",
					", 'WerkelijkVerbruik_RM_2019', 'BenaderendVerbruik_VM_2019', 'BenaderendVerbruik_VM_2019_Norm', 'AantalToegangspunten_2019'\\\r\n",
					", 'WerkelijkVerbruik_RM_2020', 'BenaderendVerbruik_VM_2020', 'BenaderendVerbruik_VM_2020_Norm', 'AantalToegangspunten_2020'\\\r\n",
					", 'WerkelijkVerbruik_RM_2021', 'BenaderendVerbruik_VM_2021', 'BenaderendVerbruik_VM_2021_Norm', 'AantalToegangspunten_2021']]\r\n",
					"\r\n",
					"# Adding the additional calculated/custom columns\r\n",
					"selected_df2['MeasurementType'] = \"Analogue\"\r\n",
					"#selected_df2['MeasurementYear'] = year\r\n",
					"#selected_df2['MeasurementMonth'] = month\r\n",
					"\r\n",
					"# convert the date column to datetime\r\n",
					"# convert the date column to datetime\r\n",
					"try:\r\n",
					"    selected_df2['Peildatum'] = pd.to_datetime(selected_df2['Peildatum'], format='%d/%b/%y')\r\n",
					"except:\r\n",
					"    try:\r\n",
					"        selected_df2['Peildatum'] = pd.to_datetime(selected_df2['Peildatum'], format='%m/%d/%y')\r\n",
					"    except:\r\n",
					"        try:\r\n",
					"            selected_df2['Peildatum'] = pd.to_datetime(selected_df2['Peildatum'], format='%d/%b/%Y')\r\n",
					"        except:\r\n",
					"            selected_df2['Peildatum'] = pd.to_datetime(selected_df2['Peildatum'], format='%d-%b-%y')\r\n",
					"\r\n",
					"# convert the datetime object to the desired format\r\n",
					"selected_df2['Peildatum'] = selected_df2['Peildatum'].dt.strftime('%d-%m-%Y')\r\n",
					"\r\n",
					"\r\n",
					"selected_df2['WerkelijkVerbruik_RM_2017'] = selected_df2['WerkelijkVerbruik_RM_2017'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['WerkelijkVerbruik_RM_2018'] = selected_df2['WerkelijkVerbruik_RM_2018'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['WerkelijkVerbruik_RM_2019'] = selected_df2['WerkelijkVerbruik_RM_2019'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['WerkelijkVerbruik_RM_2020'] = selected_df2['WerkelijkVerbruik_RM_2020'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['WerkelijkVerbruik_RM_2021'] = selected_df2['WerkelijkVerbruik_RM_2021'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['Gemiddeld_WerkelijkVerbruik'] = ((selected_df2['WerkelijkVerbruik_RM_2017'].astype(float)\\\r\n",
					"+ selected_df2['WerkelijkVerbruik_RM_2018'].astype(float) + selected_df2['WerkelijkVerbruik_RM_2019'].astype(float) + selected_df2['WerkelijkVerbruik_RM_2020'].astype(float)\\\r\n",
					"+ selected_df2['WerkelijkVerbruik_RM_2021'].astype(float))/5)\r\n",
					"\r\n",
					"selected_df2['BenaderendVerbruik_VM_2017'] = selected_df2['BenaderendVerbruik_VM_2017'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2018'] = selected_df2['BenaderendVerbruik_VM_2018'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2019'] = selected_df2['BenaderendVerbruik_VM_2019'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2020'] = selected_df2['BenaderendVerbruik_VM_2020'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2021'] = selected_df2['BenaderendVerbruik_VM_2021'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['Gemiddeld_BenaderendVerbruik'] = ((selected_df2['BenaderendVerbruik_VM_2017'].astype(float)\\\r\n",
					"+ selected_df2['BenaderendVerbruik_VM_2018'].astype(float) + selected_df2['BenaderendVerbruik_VM_2019'].astype(float) + selected_df2['BenaderendVerbruik_VM_2020'].astype(float)\\\r\n",
					"+ selected_df2['BenaderendVerbruik_VM_2021'].astype(float))/5)\r\n",
					"\r\n",
					"selected_df2['BenaderendVerbruik_VM_2017_Norm'] = selected_df2['BenaderendVerbruik_VM_2017_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2018_Norm'] = selected_df2['BenaderendVerbruik_VM_2018_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2019_Norm'] = selected_df2['BenaderendVerbruik_VM_2019_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2020_Norm'] = selected_df2['BenaderendVerbruik_VM_2020_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['BenaderendVerbruik_VM_2021_Norm'] = selected_df2['BenaderendVerbruik_VM_2021_Norm'].astype(str).str.replace(',','.')\r\n",
					"selected_df2['Gemiddeld_BenaderendVerbruik_Norm'] = ((selected_df2['BenaderendVerbruik_VM_2017_Norm'].astype(float)\\\r\n",
					"+ selected_df2['BenaderendVerbruik_VM_2018_Norm'].astype(float) + selected_df2['BenaderendVerbruik_VM_2019_Norm'].astype(float) + selected_df2['BenaderendVerbruik_VM_2020_Norm'].astype(float)\\\r\n",
					"+ selected_df2['BenaderendVerbruik_VM_2021_Norm'].astype(float))/5)\r\n",
					"\r\n",
					"selected_df2['Total_AantalToegangspunten'] = selected_df2['AantalToegangspunten_2017'].astype(float)\\\r\n",
					"+ selected_df2['AantalToegangspunten_2018'].astype(float) + selected_df2['AantalToegangspunten_2019'].astype(float) + selected_df2['AantalToegangspunten_2020'].astype(float)\\\r\n",
					"+ selected_df2['AantalToegangspunten_2021'].astype(float)\r\n",
					"\r\n",
					"# Rearrange the columns\r\n",
					"selected_df2 = selected_df2[['Trekkingsdatum', 'Peildatum', 'Markt', 'Leveringsadres_Postcode', 'Leveringsadres_Gemeente', 'WerkelijkVerbruik_RM_2017'\\\r\n",
					", 'BenaderendVerbruik_VM_2017', 'BenaderendVerbruik_VM_2017_Norm', 'AantalToegangspunten_2017', 'WerkelijkVerbruik_RM_2018', 'BenaderendVerbruik_VM_2018'\\\r\n",
					", 'BenaderendVerbruik_VM_2018_Norm', 'AantalToegangspunten_2018', 'WerkelijkVerbruik_RM_2019', 'BenaderendVerbruik_VM_2019', 'BenaderendVerbruik_VM_2019_Norm'\\\r\n",
					", 'AantalToegangspunten_2019', 'WerkelijkVerbruik_RM_2020', 'BenaderendVerbruik_VM_2020', 'BenaderendVerbruik_VM_2020_Norm', 'AantalToegangspunten_2020'\\\r\n",
					", 'WerkelijkVerbruik_RM_2021', 'BenaderendVerbruik_VM_2021', 'BenaderendVerbruik_VM_2021_Norm', 'AantalToegangspunten_2021'\\\r\n",
					",'Gemiddeld_WerkelijkVerbruik','Gemiddeld_BenaderendVerbruik','Gemiddeld_BenaderendVerbruik_Norm','Total_AantalToegangspunten'\\\r\n",
					", 'MeasurementType']] #, 'MeasurementYear', 'MeasurementMonth'"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Checking the column names\r\n",
					"selected_df2.columns"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Selecting the top 5 rows\r\n",
					"selected_df2.head()"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Checking the counts on each file\r\n",
					"print(selected_df1.shape[0]) #Analogue Reading - 2660\r\n",
					"print(selected_df2.shape[0]) #Digital Reading - 7005"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Creating a Group By to check the counts based on month, market and postal code\r\n",
					"grouped_data1 = selected_df1.groupby(['Peildatum', 'Markt', 'Leveringsadres_Postcode'])\r\n",
					"grouped_data2 = selected_df2.groupby(['Peildatum', 'Markt', 'Leveringsadres_Postcode'])\r\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"count_group_by1 = grouped_data1.agg(\r\n",
					"    total_count=('WerkelijkVerbruik_RM_2017', 'count')\r\n",
					")\r\n",
					"count_group_by1.head() #Analogue Reading Group BY\r\n",
					""
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"count_group_by2 = grouped_data2.agg(\r\n",
					"    total_count=('WerkelijkVerbruik_RM_2017', 'count')\r\n",
					")\r\n",
					"count_group_by2.head() #Digital Reading Group By"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Doing union operation to get a single dataframe combining Analogue and Digital measurement data\r\n",
					"df = pd.concat([selected_df2, selected_df1])"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df.head(5)"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.dtypes"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark_df = spark.createDataFrame(df)"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"file_path = 'abfss://output@rawdevstorage.dfs.core.windows.net/Electricity_Measurement_pandas_union'\r\n",
					"write_format=\"delta\"\r\n",
					"partition_by=[\"Peildatum\"]\r\n",
					"mode_ = \"append\" #overwrite #append\r\n",
					"\r\n",
					"spark_df\\\r\n",
					".write\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".option(\"overwriteSchema\", \"true\")\\\r\n",
					".format(write_format)\\\r\n",
					".partitionBy(partition_by)\\\r\n",
					".mode(mode_)\\\r\n",
					".save(file_path)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Renaming the column names before join in order to avoid ambigious column names after the join\r\n",
					"selected_df1 = selected_df1.add_prefix('AM_')\r\n",
					"selected_df2 = selected_df2.add_prefix('DM_')\r\n",
					""
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Doing a join operation to get a new data frame\r\n",
					"join_df = pd.merge(selected_df2, selected_df1, left_on=['DM_Markt', 'DM_Leveringsadres_Postcode','DM_Leveringsadres_Gemeente','DM_Peildatum'], right_on=['AM_Markt', 'AM_Leveringsadres_Postcode','AM_Leveringsadres_Gemeente','AM_Peildatum'], how='outer')\r\n",
					"\r\n",
					"\r\n",
					"# # Selecting the required columns after the join operation\r\n",
					"# join_df = join_df[['DM_Trekkingsdatum', 'DM_Peildatum', 'DM_Markt', 'DM_Leveringsadres_Postcode', 'DM_Leveringsadres_Gemeente'\\\r\n",
					"# , 'DM_WerkelijkVerbruik_RM_2017', 'DM_BenaderendVerbruik_VM_2017', 'DM_BenaderendVerbruik_VM_2017_Norm', 'DM_AantalToegangspunten_2017'\\\r\n",
					"# , 'AM_WerkelijkVerbruik_RM_2017', 'AM_BenaderendVerbruik_VM_2017', 'AM_BenaderendVerbruik_VM_2017_Norm', 'AM_AantalToegangspunten_2017'\\\r\n",
					"# , 'DM_WerkelijkVerbruik_RM_2018', 'DM_BenaderendVerbruik_VM_2018', 'DM_BenaderendVerbruik_VM_2018_Norm', 'DM_AantalToegangspunten_2018'\\\r\n",
					"# , 'AM_WerkelijkVerbruik_RM_2018', 'AM_BenaderendVerbruik_VM_2018', 'AM_BenaderendVerbruik_VM_2018_Norm', 'AM_AantalToegangspunten_2018'\\\r\n",
					"# , 'DM_WerkelijkVerbruik_RM_2019', 'DM_BenaderendVerbruik_VM_2019', 'DM_BenaderendVerbruik_VM_2019_Norm', 'DM_AantalToegangspunten_2019'\\\r\n",
					"# , 'AM_WerkelijkVerbruik_RM_2019', 'AM_BenaderendVerbruik_VM_2019', 'AM_BenaderendVerbruik_VM_2019_Norm', 'AM_AantalToegangspunten_2019'\\\r\n",
					"# , 'DM_WerkelijkVerbruik_RM_2020', 'DM_BenaderendVerbruik_VM_2020', 'DM_BenaderendVerbruik_VM_2020_Norm', 'DM_AantalToegangspunten_2020'\\\r\n",
					"# , 'AM_WerkelijkVerbruik_RM_2020', 'AM_BenaderendVerbruik_VM_2020', 'AM_BenaderendVerbruik_VM_2020_Norm', 'AM_AantalToegangspunten_2020'\\\r\n",
					"# , 'DM_WerkelijkVerbruik_RM_2021', 'DM_BenaderendVerbruik_VM_2021', 'DM_BenaderendVerbruik_VM_2021_Norm', 'DM_AantalToegangspunten_2021'\\\r\n",
					"# , 'AM_WerkelijkVerbruik_RM_2021', 'AM_BenaderendVerbruik_VM_2021', 'AM_BenaderendVerbruik_VM_2021_Norm', 'AM_AantalToegangspunten_2021'\\\r\n",
					"# , 'DM_Gemiddeld_WerkelijkVerbruik','DM_Gemiddeld_BenaderendVerbruik','DM_Gemiddeld_BenaderendVerbruik_Norm','DM_Total_AantalToegangspunten'\\\r\n",
					"# , 'AM_Gemiddeld_WerkelijkVerbruik','AM_Gemiddeld_BenaderendVerbruik','AM_Gemiddeld_BenaderendVerbruik_Norm','AM_Total_AantalToegangspunten'\r\n",
					"# , 'DM_MeasurementType']] #, 'DM_MeasurementYear', 'DM_MeasurementMonth'"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Printing the count of the join data frame\r\n",
					"join_df.shape[0]\r\n",
					""
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Displaying the top 5 rows\r\n",
					"join_df.head(5)"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark_join_df = spark.createDataFrame(join_df)"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"file_path = 'abfss://output@rawdevstorage.dfs.core.windows.net/Electricity_Measurement_pandas_join'\r\n",
					"write_format=\"delta\"\r\n",
					"partition_by=[\"DM_Peildatum\"]\r\n",
					"mode_ = \"append\" #overwrite #append\r\n",
					"\r\n",
					"spark_join_df\\\r\n",
					".write\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".option(\"overwriteSchema\", \"true\")\\\r\n",
					".format(write_format)\\\r\n",
					".partitionBy(partition_by)\\\r\n",
					".mode(mode_)\\\r\n",
					".save(file_path)"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}